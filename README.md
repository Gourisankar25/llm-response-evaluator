# ğŸ§  LLM Response Evaluation Framework

A production-ready system for evaluating LLM responses across 5 key metrics: hallucination detection, groundedness, faithfulness, context relevance, and answer completeness.

## ğŸ“Š Evaluation Metrics

1. **Hallucination Detection** - Detects contradictions and fabricated information using NLI models
2. **Groundedness Score** - Measures % of claims supported by source context
3. **Faithfulness Score** - Calculates semantic similarity between response and context
4. **Context Relevance** - Scores how relevant the provided context is to the question
5. **Answer Completeness** - Checks if the answer addresses all aspects of the question

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI
- **Frontend**: Streamlit
- **Database**: SQLite (can upgrade to SQL Server)
- **ML/NLP**: HuggingFace Transformers, SentenceTransformers
- **Testing**: pytest
- **CI/CD**: GitHub Actions

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### 2. Set Up Environment

Create a `.env` file:
```
DATABASE_URL=sqlite:///./llm_eval.db
MODEL_CACHE_DIR=./models
LOG_LEVEL=INFO
```

### 3. Run the API

```bash
python -m uvicorn app.main:app --reload
```

API will be available at: http://localhost:8000
API docs at: http://localhost:8000/docs

### 4. Run the Dashboard

```bash
streamlit run dashboard/app.py
```

Dashboard will open at: http://localhost:8501

## ğŸ“ Project Structure

```
LLM_eval/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”œâ”€â”€ database.py          # Database setup
â”‚   â””â”€â”€ routers/
â”‚       â””â”€â”€ evaluation.py    # Evaluation endpoints
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hallucination.py     # Hallucination detector
â”‚   â”œâ”€â”€ groundedness.py      # Groundedness scorer
â”‚   â”œâ”€â”€ faithfulness.py      # Faithfulness scorer
â”‚   â”œâ”€â”€ relevance.py         # Context relevance checker
â”‚   â”œâ”€â”€ completeness.py      # Answer completeness checker
â”‚   â””â”€â”€ pipeline.py          # Main evaluation pipeline
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py               # Streamlit dashboard
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_evaluators.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=evaluators --cov=app
```

## ğŸ“š How It Works

### Hallucination Detection (NLI)
Uses Natural Language Inference models to detect contradictions:
- **Premise**: Source context
- **Hypothesis**: LLM response claim
- **Output**: Entailment/Neutral/Contradiction

### Groundedness Scoring
1. Extracts atomic claims from the response
2. Checks each claim against source context
3. Calculates % of grounded claims

### Faithfulness Scoring
1. Generates embeddings for response and context
2. Calculates cosine similarity
3. Higher score = more faithful to source

### Context Relevance
Uses cross-encoder models to score how relevant the context is to answering the question.

### Answer Completeness
1. Identifies key aspects in the question
2. Checks if each aspect is addressed in the answer
3. Calculates completeness percentage

## ğŸ”„ API Usage

```python
import requests

response = requests.post("http://localhost:8000/api/evaluate", json={
    "question": "What is the capital of France?",
    "context": "France is a country in Europe. Paris is its capital city.",
    "llm_response": "The capital of France is Paris."
})

print(response.json())
```

## ğŸ“ˆ Roadmap

- [x] Phase 1: Core evaluation engine
- [x] Phase 2: FastAPI integration
- [x] Phase 3: SQLite database
- [x] Phase 4: Streamlit dashboard
- [ ] Phase 5: Advanced caching
- [ ] Phase 6: SQL Server migration
- [ ] Phase 7: Docker containerization

## ğŸ¤ Contributing

This is a learning project! Feel free to experiment and improve.

## ğŸ“ License

MIT License
