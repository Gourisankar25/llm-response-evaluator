"""
Pydantic Models for API Request/Response validation

These models define the structure of data that flows through the API.
Pydantic automatically validates data and provides type checking.

Why Pydantic?
- Automatic validation (ensures correct data types)
- Clear API documentation (FastAPI uses these for docs)
- Serialization (converts Python objects to JSON)
- Type hints (IDE autocomplete support)

Example:
    # This would raise a validation error:
    EvaluationRequest(question=123)  # Error: expected str, got int
    
    # This is correct:
    EvaluationRequest(question="What is AI?", context="...", llm_response="...")
"""

from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional
from datetime import datetime


class EvaluationRequest(BaseModel):
    """
    Request model for LLM response evaluation.
    
    This defines what data the API expects when you call /api/evaluate
    """
    question: str = Field(
        ...,  # Required field
        min_length=5,
        max_length=1000,
        description="The question that was asked to the LLM",
        example="What is machine learning?"
    )
    
    context: str = Field(
        ...,
        min_length=10,
        max_length=10000,
        description="Source context provided to the LLM",
        example="Machine learning is a subset of AI that enables systems to learn from data."
    )
    
    llm_response: str = Field(
        ...,
        min_length=10,
        max_length=5000,
        description="The response generated by the LLM",
        example="Machine learning allows computers to learn patterns from data without explicit programming."
    )
    
    hallucination_threshold: Optional[float] = Field(
        default=0.3,
        ge=0.0,  # Greater than or equal to 0
        le=1.0,  # Less than or equal to 1
        description="Threshold for hallucination detection (0-1)"
    )
    
    groundedness_threshold: Optional[float] = Field(
        default=0.4,
        ge=0.0,
        le=1.0,
        description="Threshold for groundedness scoring (0-1)"
    )
    
    relevance_threshold: Optional[float] = Field(
        default=0.5,
        ge=0.0,
        le=1.0,
        description="Threshold for context relevance (0-1)"
    )
    
    @validator('question', 'context', 'llm_response')
    def not_empty(cls, v):
        """Ensure fields are not just whitespace"""
        if not v.strip():
            raise ValueError("Field cannot be empty or whitespace only")
        return v.strip()
    
    class Config:
        json_schema_extra = {
            "example": {
                "question": "What is the capital of France?",
                "context": "France is a country in Western Europe. Paris is its capital and largest city.",
                "llm_response": "The capital of France is Paris.",
                "hallucination_threshold": 0.3,
                "groundedness_threshold": 0.4,
                "relevance_threshold": 0.5
            }
        }


class MetricResult(BaseModel):
    """Result for an individual evaluation metric"""
    score: float = Field(description="Metric score (0-1)")
    weight: float = Field(description="Weight in overall score")


class HallucinationMetric(MetricResult):
    """Hallucination detection results"""
    has_hallucination: bool
    contradiction_score: float
    label: str


class GroundednessMetric(MetricResult):
    """Groundedness scoring results"""
    num_claims: int
    num_supported: int
    unsupported_claims: List[str]


class FaithfulnessMetric(MetricResult):
    """Faithfulness scoring results"""
    similarity: float


class RelevanceMetric(MetricResult):
    """Context relevance results"""
    is_relevant: bool
    interpretation: str


class CompletenessMetric(MetricResult):
    """Answer completeness results"""
    num_aspects: int
    num_addressed: int
    missing_aspects: List[str]


class EvaluationResponse(BaseModel):
    """
    Response model for evaluation results.
    
    This is what the API returns after evaluating an LLM response.
    """
    overall_score: float = Field(description="Overall quality score (0-1)")
    overall_score_100: float = Field(description="Overall score on 0-100 scale")
    quality_tier: str = Field(description="Quality category (Excellent/Good/Acceptable/Poor/Very Poor)")
    emoji: str = Field(description="Visual indicator of quality")
    
    hallucination: HallucinationMetric
    groundedness: GroundednessMetric
    faithfulness: FaithfulnessMetric
    relevance: RelevanceMetric
    completeness: CompletenessMetric
    
    issues: List[str] = Field(description="List of identified issues")
    has_issues: bool = Field(description="Whether any issues were found")
    
    weights: Dict[str, float] = Field(description="Weights used for each metric")
    execution_time_seconds: float = Field(description="Time taken for evaluation")
    
    class Config:
        json_schema_extra = {
            "example": {
                "overall_score": 0.85,
                "overall_score_100": 85.0,
                "quality_tier": "Good",
                "emoji": "âœ…",
                "issues": [],
                "has_issues": False,
                "execution_time_seconds": 2.5
            }
        }


class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    message: str
    timestamp: datetime
    models_loaded: bool


class ErrorResponse(BaseModel):
    """Error response model"""
    error: str
    detail: str
    status_code: int
